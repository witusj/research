<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="LNMB">

<title>Markov Decision Processes - Week 5</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="course-mdp-wk5_files/libs/clipboard/clipboard.min.js"></script>
<script src="course-mdp-wk5_files/libs/quarto-html/quarto.js"></script>
<script src="course-mdp-wk5_files/libs/quarto-html/popper.min.js"></script>
<script src="course-mdp-wk5_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="course-mdp-wk5_files/libs/quarto-html/anchor.min.js"></script>
<link href="course-mdp-wk5_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="course-mdp-wk5_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="course-mdp-wk5_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="course-mdp-wk5_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="course-mdp-wk5_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Markov Decision Processes - Week 5</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>LNMB </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="total-reward-markov-decision-processes-mdps" class="level1">
<h1>Total Reward Markov Decision Processes (MDPs)</h1>
<section id="characteristics-and-basic-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="characteristics-and-basic-assumptions">Characteristics and Basic Assumptions</h2>
<p>We consider a Markov Decision Process (MDP) with the following features:</p>
<ul>
<li><p><strong>State Space (<span class="math inline">\(S\)</span>)</strong>: A discrete, countable set of states.</p></li>
<li><p><strong>Actions (<span class="math inline">\(A(i)\)</span>)</strong>: For each state <span class="math inline">\(i \in S\)</span>, there is a finite set of available actions.</p></li>
<li><p><strong>Transition Probabilities and Rewards</strong>:</p>
<ul>
<li><strong>Transition Probabilities (<span class="math inline">\(P_{ij}(a)\)</span>)</strong>: The probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> when action <span class="math inline">\(a\)</span> is taken.</li>
<li><strong>Rewards (<span class="math inline">\(r_i(a)\)</span>)</strong>: The immediate reward received when action <span class="math inline">\(a\)</span> is taken in state <span class="math inline">\(i\)</span>.</li>
<li>These functions are <strong>stationary</strong>, meaning they do not change over time.</li>
<li>They satisfy <strong>Puterman’s Condition 2.6.10</strong>, ensuring certain regularity conditions (refer to Puterman’s textbook on MDPs for details).</li>
</ul></li>
<li><p><strong>Sink State (<span class="math inline">\(0\)</span>)</strong>:</p>
<ul>
<li>There exists a special state <span class="math inline">\(0 \in S\)</span> known as the sink or absorbing state.</li>
<li><strong>Actions</strong>: The only available action in state <span class="math inline">\(0\)</span> is <span class="math inline">\(0\)</span>, i.e., <span class="math inline">\(A(0) = \{0\}\)</span>.</li>
<li><strong>Transition</strong>: The process remains in state <span class="math inline">\(0\)</span> once it reaches it, <span class="math inline">\(P_{00}(0) = 1\)</span>.</li>
<li><strong>Reward</strong>: The reward in state <span class="math inline">\(0\)</span> is zero, <span class="math inline">\(r_0(0) = 0\)</span>.</li>
</ul></li>
<li><p><strong>Boundedness Condition</strong>:</p>
<ul>
<li>There exists a function <span class="math inline">\(M: S \rightarrow \mathbb{R}^+\)</span> with <span class="math inline">\(M(0) = 1\)</span> such that for all <span class="math inline">\(i \in S \setminus \{0\}\)</span> and <span class="math inline">\(a \in A(i)\)</span>: <span class="math display">\[
\sum_{j \in S} P_{ij}(a) M(j) \leq \beta M(i),
\]</span> where <span class="math inline">\(\beta = \sup_{a \in A(i)} \| P(a) \|\)</span> and <span class="math inline">\(\| P(a) \|\)</span> is the matrix norm defined as: <span class="math display">\[
\| P(a) \| = \sup_{\| g \| \leq 1} \| P(a) g \|,
\]</span> with <span class="math inline">\(\| g \| = \sup_{i \in S} |g(i)|\)</span> for <span class="math inline">\(g: S \rightarrow \mathbb{R}\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="algorithms-for-solving-mdps" class="level2">
<h2 class="anchored" data-anchor-id="algorithms-for-solving-mdps">Algorithms for Solving MDPs</h2>
<ol type="1">
<li><strong>Policy Iteration</strong>:
<ul>
<li>Iteratively improve policies until an optimal policy is found.</li>
<li>Focus of the current discussion.</li>
</ul></li>
<li><strong>Value Iteration (Successive Approximation)</strong>:
<ul>
<li>Iteratively update value functions until convergence.</li>
<li>To be discussed separately.</li>
</ul></li>
</ol>
</section>
</section>
<section id="linear-programming-formulation-of-finite-mdps" class="level1">
<h1>Linear Programming Formulation of Finite MDPs</h1>
<p>For finite MDPs, the problem can be formulated as a linear program (LP). This approach is useful for understanding the structure of optimal policies and for computational purposes.</p>
<section id="superharmonic-functions" class="level2">
<h2 class="anchored" data-anchor-id="superharmonic-functions">Superharmonic Functions</h2>
<p>A function <span class="math inline">\(v: S \rightarrow \mathbb{R}\)</span> is called <strong>superharmonic</strong> if it satisfies: <span class="math display">\[
v(i) \leq r_i(a) + \sum_{j \in S} P_{ij}(a) v(j), \quad \forall a \in A(i), \quad \forall i \in S \setminus \{0\}.
\]</span> The goal is to find the smallest superharmonic function <span class="math inline">\(v\)</span>, which corresponds to the optimal value function <span class="math inline">\(v^*\)</span>.</p>
</section>
<section id="primal-linear-program-lp" class="level2">
<h2 class="anchored" data-anchor-id="primal-linear-program-lp">Primal Linear Program (LP)</h2>
<p>The primal LP aims to minimize a weighted sum of the value function over the states: - <strong>Objective</strong>: <span class="math display">\[
  \text{Minimize} \quad \sum_{i \in S \setminus \{0\}} \beta_i v(i),
  \]</span> where <span class="math inline">\(\beta_i &gt; 0\)</span> are given weights.</p>
<ul>
<li><strong>Constraints</strong>: <span class="math display">\[
v(i) \leq r_i(a) + \sum_{j \in S} P_{ij}(a) v(j), \quad \forall a \in A(i), \quad \forall i \in S \setminus \{0\}.
\]</span></li>
</ul>
</section>
<section id="dual-linear-program-dlp" class="level2">
<h2 class="anchored" data-anchor-id="dual-linear-program-dlp">Dual Linear Program (DLP)</h2>
<p>The dual LP corresponds to maximizing the expected total reward, with variables representing the expected number of times each action is taken in each state:</p>
<ul>
<li><p><strong>Variables</strong>: <span class="math inline">\(X_i(a) \geq 0\)</span>, representing occupation measures.</p></li>
<li><p><strong>Objective</strong>: <span class="math display">\[
\text{Maximize} \quad \sum_{(i,a)} r_i(a) X_i(a).
\]</span></p></li>
<li><p><strong>Constraints</strong>: <span class="math display">\[
\sum_{(i,a)} (\delta_{ij} - P_{ij}(a)) X_i(a) = \beta_j, \quad \forall j \in S \setminus \{0\},
\]</span> where <span class="math inline">\(\delta_{ij}\)</span> is the Kronecker delta (<span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(i = j\)</span>, <span class="math inline">\(0\)</span> otherwise).</p></li>
</ul>
</section>
<section id="interpretation-of-dual-variables" class="level2">
<h2 class="anchored" data-anchor-id="interpretation-of-dual-variables">Interpretation of Dual Variables</h2>
<ul>
<li><strong>Occupation Measures (<span class="math inline">\(X_i(a)\)</span>)</strong>:
<ul>
<li>Represent the expected total number of times action <span class="math inline">\(a\)</span> is taken in state <span class="math inline">\(i\)</span> before reaching the sink state.</li>
<li>Can be interpreted in terms of the stationary distribution under a policy.</li>
</ul></li>
</ul>
</section>
<section id="relationship-between-primal-and-dual-lps" class="level2">
<h2 class="anchored" data-anchor-id="relationship-between-primal-and-dual-lps">Relationship Between Primal and Dual LPs</h2>
<p><strong>Theorem</strong>:</p>
<ol type="1">
<li><strong>Optimal Solutions Correspond</strong>:
<ul>
<li>The optimal value of the primal LP equals that of the dual LP.</li>
<li>The optimal <span class="math inline">\(v^*\)</span> from the primal LP and <span class="math inline">\(X^*\)</span> from the dual LP satisfy the complementary slackness conditions.</li>
</ul></li>
<li><strong>Policy Interpretation</strong>:
<ul>
<li>The optimal <span class="math inline">\(X^*\)</span> corresponds to a policy <span class="math inline">\(f^*\)</span> in the set of deterministic Markov policies <span class="math inline">\(D_{MD}\)</span>.</li>
</ul></li>
<li><strong>Structure of Solutions</strong>:
<ul>
<li>The number of constraints in the dual LP is <span class="math inline">\(|S \setminus \{0\}|\)</span>.</li>
<li>At optimality, there are fewer than <span class="math inline">\(|S \setminus \{0\}|\)</span> positive <span class="math inline">\(X_i(a)\)</span>, corresponding to deterministic policies.</li>
</ul></li>
</ol>
</section>
</section>
<section id="example-discounted-mdp" class="level1">
<h1>Example: Discounted MDP</h1>
<p>Consider a discounted MDP with the following parameters:</p>
<ul>
<li><p><strong>States</strong>: <span class="math inline">\(S = \{1, 2, 3, 0\}\)</span>, where <span class="math inline">\(0\)</span> is the sink state.</p></li>
<li><p><strong>Actions</strong>: <span class="math inline">\(A_i = \{1, 2, 3\}\)</span> for <span class="math inline">\(i \in \{1, 2, 3\}\)</span>.</p></li>
<li><p><strong>Rewards</strong>:</p>
<ul>
<li><span class="math inline">\(r(1) = 9\)</span>.</li>
<li><span class="math inline">\(r(2) = 6\)</span>.</li>
<li><span class="math inline">\(r(3) = 3\)</span>.</li>
</ul></li>
<li><p><strong>Discount Factor</strong>: <span class="math inline">\(\beta = \frac{1}{4}\)</span>.</p></li>
<li><p><strong>Transition Probabilities</strong>: Defined according to the problem (not specified in the given notes).</p></li>
</ul>
<p>We can set up the dual LP for this MDP and solve for the occupation measures <span class="math inline">\(X_i(a)\)</span> to find the optimal policy.</p>
</section>
<section id="optimal-stopping-problems" class="level1">
<h1>Optimal Stopping Problems</h1>
<p>An optimal stopping problem is a special case of an MDP where at each state, the decision is whether to continue or to stop. The goal is to maximize the expected reward by choosing the optimal stopping time.</p>
<section id="general-setup" class="level2">
<h2 class="anchored" data-anchor-id="general-setup">General Setup</h2>
<ul>
<li><p><strong>States</strong>: Finite set <span class="math inline">\(S\)</span>.</p></li>
<li><p><strong>Actions</strong>:</p>
<ul>
<li><strong>Continue (C)</strong>: Pay a cost <span class="math inline">\(c\)</span>, and move to a new state according to transition probabilities <span class="math inline">\(P_{ij}\)</span>.</li>
<li><strong>Stop (S)</strong>: Receive an immediate reward <span class="math inline">\(r_i\)</span>, and move to the sink state <span class="math inline">\(0\)</span>.</li>
</ul></li>
<li><p><strong>Sink State</strong>:</p>
<ul>
<li>Once in state <span class="math inline">\(0\)</span>, the process remains there with zero reward.</li>
</ul></li>
</ul>
</section>
<section id="mdp-formulation" class="level2">
<h2 class="anchored" data-anchor-id="mdp-formulation">MDP Formulation</h2>
<ul>
<li><strong>Transition Probabilities</strong>:
<ul>
<li><span class="math inline">\(P_{ij}(C) = P_{ij}\)</span> for all <span class="math inline">\(i, j \in S\)</span>.</li>
<li><span class="math inline">\(P_{i0}(S) = 1\)</span> for all <span class="math inline">\(i \in S\)</span>.</li>
<li><span class="math inline">\(P_{00}(0) = 1\)</span>.</li>
</ul></li>
<li><strong>Rewards</strong>:
<ul>
<li><span class="math inline">\(r_i(C) = -c\)</span> (cost of continuing).</li>
<li><span class="math inline">\(r_i(S) = r_i\)</span> (reward for stopping).</li>
</ul></li>
</ul>
</section>
<section id="control-limit-policy" class="level2">
<h2 class="anchored" data-anchor-id="control-limit-policy">Control-Limit Policy</h2>
<p>Under certain conditions, the optimal policy is of <strong>control-limit type</strong>:</p>
<ul>
<li>There exists a threshold <span class="math inline">\(t\)</span> such that:
<ul>
<li><strong>Stop</strong> when the state <span class="math inline">\(i\)</span> satisfies <span class="math inline">\(i \geq t\)</span>.</li>
<li><strong>Continue</strong> when <span class="math inline">\(i &lt; t\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="example-house-selling-problem" class="level2">
<h2 class="anchored" data-anchor-id="example-house-selling-problem">Example: House Selling Problem</h2>
<ul>
<li><strong>Scenario</strong>:
<ul>
<li>A seller receives daily offers for a house.</li>
<li>Each day, the seller can accept (stop) or reject (continue) the current offer.</li>
<li>Continuing incurs a daily cost <span class="math inline">\(c\)</span>.</li>
</ul></li>
<li><strong>MDP Formulation</strong>:
<ul>
<li><strong>States</strong>: Possible offer amounts.</li>
<li><strong>Actions</strong>:
<ul>
<li><strong>Accept (S)</strong>: Sell the house at the current offer.</li>
<li><strong>Reject (C)</strong>: Pay cost <span class="math inline">\(c\)</span>, and receive a new offer the next day.</li>
</ul></li>
<li><strong>Transition Probabilities</strong>: Determined by the distribution of offers.</li>
</ul></li>
<li><strong>Optimal Policy</strong>:
<ul>
<li>A control-limit policy where the seller accepts any offer above a certain threshold.</li>
</ul></li>
</ul>
</section>
</section>
<section id="example-teds-game-with-soldiers" class="level1">
<h1>Example: Ted’s Game with Soldiers</h1>
<p><strong>Scenario</strong>:</p>
<ul>
<li>Ted and 50 soldiers each write down a unique real number.</li>
<li>Ted draws the numbers one by one.</li>
<li>He can claim that the most recent number is the highest so far.</li>
<li>If he is correct, he wins $2 from each soldier.</li>
<li>If he is wrong, he pays $0.75 to each soldier.</li>
<li>The game is repeated many times, and Ted wants to maximize his expected gain.</li>
</ul>
<section id="formulating-as-an-optimal-stopping-problem" class="level2">
<h2 class="anchored" data-anchor-id="formulating-as-an-optimal-stopping-problem">Formulating as an Optimal Stopping Problem</h2>
<ul>
<li><strong>States</strong>:
<ul>
<li><span class="math inline">\(S = \{0, 1, 2, \dots, N, N+1\}\)</span>, where <span class="math inline">\(N = 50\)</span>.</li>
<li>State <span class="math inline">\(i\)</span> indicates that <span class="math inline">\(i\)</span> numbers have been drawn, and the last number is the highest so far.</li>
<li>State <span class="math inline">\(N+1\)</span> indicates that all numbers have been drawn without finding the highest number.</li>
</ul></li>
<li><strong>Actions</strong>:
<ul>
<li><strong>Stop</strong>: Claim the current number is the highest.</li>
<li><strong>Continue</strong>: Draw the next number.</li>
</ul></li>
<li><strong>Transition Probabilities</strong>:
<ul>
<li>The probability that the <span class="math inline">\(i\)</span>-th number is the highest so far is <span class="math inline">\(\frac{1}{i}\)</span>.</li>
<li>The probability of moving from state <span class="math inline">\(i\)</span> to <span class="math inline">\(i+1\)</span> without stopping is <span class="math inline">\(1 - \frac{1}{i}\)</span>.</li>
</ul></li>
<li><strong>Rewards</strong>:
<ul>
<li><strong>Correct Stop</strong>: Gain <span class="math inline">\(G = 50 \times \$2 = \$100\)</span>.</li>
<li><strong>Incorrect Stop</strong>: Lose <span class="math inline">\(L = 50 \times \$0.75 = \$37.50\)</span>.</li>
<li><strong>Continue</strong>: No immediate gain or loss.</li>
</ul></li>
</ul>
</section>
<section id="optimal-policy" class="level2">
<h2 class="anchored" data-anchor-id="optimal-policy">Optimal Policy</h2>
<p>Ted’s optimal strategy resembles the solution to the <strong>secretary problem</strong>:</p>
<ul>
<li><strong>Skip a certain number of initial draws</strong> (do not stop during these).</li>
<li><strong>After the initial skips</strong>, stop at the first number that is higher than all previously observed numbers.</li>
<li>The optimal number of initial skips <span class="math inline">\(k\)</span> can be found by maximizing the probability of success.</li>
</ul>
</section>
<section id="calculations" class="level2">
<h2 class="anchored" data-anchor-id="calculations">Calculations</h2>
<ul>
<li><strong>Probability of Winning</strong>:
<ul>
<li>The probability that the highest number is among the last <span class="math inline">\(N - k\)</span> numbers and that it is the first maximum after <span class="math inline">\(k\)</span> draws.</li>
<li>This probability is approximately <span class="math inline">\(\frac{1}{e}\)</span> when <span class="math inline">\(k = \frac{N}{e}\)</span>.</li>
</ul></li>
<li><strong>Expected Gain</strong>:
<ul>
<li><strong>Expected Gain</strong> <span class="math inline">\(=\)</span> (Probability of Winning <span class="math inline">\(\times G\)</span>) <span class="math inline">\(-\)</span> (Probability of Losing <span class="math inline">\(\times L\)</span>).</li>
<li>Ted needs to compute this expected gain and choose <span class="math inline">\(k\)</span> to maximize it.</li>
</ul></li>
</ul>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>The MDP framework allows us to model and solve complex decision-making problems involving uncertainty and time. By formulating the problem appropriately, we can use tools like linear programming, policy iteration, and value iteration to find optimal strategies.</p>
<p>Key takeaways:</p>
<ul>
<li><p><strong>Superharmonic Functions</strong>: Essential in formulating the LP for MDPs.</p></li>
<li><p><strong>Duality</strong>: The dual LP provides valuable insights into the structure of optimal policies.</p></li>
<li><p><strong>Control-Limit Policies</strong>: Common in optimal stopping problems; policies are determined by thresholds.</p></li>
<li><p><strong>Optimal Stopping</strong>: Problems like Ted’s game illustrate the application of MDPs in real-world scenarios.</p></li>
</ul>
<p>Understanding these concepts equips us with the tools to tackle a wide range of decision-making problems in operations research, economics, and beyond.</p>
<hr>
<p><strong>Note</strong>: This summary is based on advanced topics in Markov Decision Processes. For detailed explanations and proofs, refer to textbooks such as “Markov Decision Processes: Discrete Stochastic Dynamic Programming” by Martin L. Puterman.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>