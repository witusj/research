<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>course-mdp-wk1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="course-mdp-wk1_files/libs/clipboard/clipboard.min.js"></script>
<script src="course-mdp-wk1_files/libs/quarto-html/quarto.js"></script>
<script src="course-mdp-wk1_files/libs/quarto-html/popper.min.js"></script>
<script src="course-mdp-wk1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="course-mdp-wk1_files/libs/quarto-html/anchor.min.js"></script>
<link href="course-mdp-wk1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="course-mdp-wk1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="course-mdp-wk1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="course-mdp-wk1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="course-mdp-wk1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>


  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Markov Decision Processes: Finite Horizon Decision Problems</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Markov Decision Processes (MDPs) are a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. This tutorial covers finite horizon decision problems, which are MDPs with a predetermined, finite number of decision steps.</p>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<ol type="1">
<li>States</li>
<li>Actions</li>
<li>Transition Probabilities</li>
<li>Rewards</li>
<li>Policies</li>
<li>Value Functions</li>
<li>Bellman Optimality Principle</li>
</ol>
</section>
<section id="the-model" class="level2">
<h2 class="anchored" data-anchor-id="the-model">The Model</h2>
<p>An MDP consists of:</p>
<ul>
<li>A set of states <span class="math inline">\(S\)</span></li>
<li>A set of actions <span class="math inline">\(A(s)\)</span> for each state <span class="math inline">\(s\)</span>, which may depend on both the state and time</li>
<li>Transition probabilities <span class="math inline">\(P(s'|s,a)\)</span> for moving from state <span class="math inline">\(s\)</span> to <span class="math inline">\(s'\)</span> when taking action <span class="math inline">\(a\)</span></li>
<li>Rewards <span class="math inline">\(R(s,a,s')\)</span> for taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and moving to state <span class="math inline">\(s'\)</span></li>
<li>For multi-step, infinite horizon problems, a discount factor <span class="math inline">\(\gamma\)</span> (0 ≤ <span class="math inline">\(\gamma\)</span> ≤ 1) may be introduced. However, for finite horizon problems, the objective is to maximize the total reward without discounting, across a set time horizon <span class="math inline">\(T\)</span>.</li>
</ul>
<p>For finite horizon problems, we also have: - A finite time horizon <span class="math inline">\(T\)</span></p>
</section>
<section id="example-investment-decision" class="level2">
<h2 class="anchored" data-anchor-id="example-investment-decision">Example: Investment Decision</h2>
<p>Let’s consider a simple investment example to illustrate these concepts.</p>
<p>An investor has $10,000 to invest in either Company A or Company B stocks for 5 years. Each year, they can choose to invest in either company.</p>
<ul>
<li><strong>Company A</strong>: 10% chance of doubling the investment, 90% chance of no change</li>
<li><strong>Company B</strong>: 60% chance of doubling the investment, 40% chance of losing everything</li>
</ul>
<p>The goal is to maximize the expected total return after 5 years.</p>
<p>Let’s model this as an MDP:</p>
<div id="183b7ef6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotly.subplots <span class="im">import</span> make_subplots</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># States: Amount of money (in thousands of dollars)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">51</span>))  <span class="co"># $0 to $50,000</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Actions: 0 for Company A, 1 for Company B</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Transition probabilities</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transition_prob(s, a, s_next):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> a <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># Company A</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s_next <span class="op">==</span> s:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="fl">0.9</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> s_next <span class="op">==</span> <span class="dv">2</span> <span class="op">*</span> s:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="fl">0.1</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> a <span class="op">==</span> <span class="dv">1</span>:  <span class="co"># Company B</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> s_next <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="fl">0.4</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> s_next <span class="op">==</span> <span class="dv">2</span> <span class="op">*</span> s:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="fl">0.6</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Reward function: Simplified as the difference in investment states</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reward(s, a, s_next):</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s_next <span class="op">-</span> s</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Time horizon</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="value-function-and-optimal-policy" class="level2">
<h2 class="anchored" data-anchor-id="value-function-and-optimal-policy">Value Function and Optimal Policy</h2>
<p>The value function <span class="math inline">\(V_t(s)\)</span> represents the expected total reward from state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span>, following the optimal policy from <span class="math inline">\(t\)</span> onwards. We can compute this using dynamic programming and the Bellman optimality principle, which simplifies complex decision problems by considering that the solution to the entire problem involves solving smaller sub-problems recursively.</p>
<div id="fb4dddfe" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_value_function(T):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((T<span class="op">+</span><span class="dv">1</span>, <span class="bu">len</span>(states)))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> np.zeros((T, <span class="bu">len</span>(states)), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s <span class="kw">in</span> states:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            max_value <span class="op">=</span> <span class="bu">float</span>(<span class="st">'-inf'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            best_action <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> a <span class="kw">in</span> actions:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                value <span class="op">=</span> <span class="bu">sum</span>(transition_prob(s, a, s_next) <span class="op">*</span> </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                            (reward(s, a, s_next) <span class="op">+</span> V[t<span class="op">+</span><span class="dv">1</span>][s_next])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> s_next <span class="kw">in</span> states)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> value <span class="op">&gt;</span> max_value:</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                    max_value <span class="op">=</span> value</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                    best_action <span class="op">=</span> a</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            V[t][s] <span class="op">=</span> max_value</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            policy[t][s] <span class="op">=</span> best_action</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V, policy</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>V, policy <span class="op">=</span> compute_value_function(T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-the-results" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-results">Visualizing the Results</h2>
<p>Let’s create interactive visualizations of the optimal policy and value function using Plotly:</p>
<div id="78e8280a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create subplots</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> make_subplots(rows<span class="op">=</span><span class="dv">1</span>, cols<span class="op">=</span><span class="dv">2</span>, subplot_titles<span class="op">=</span>(<span class="st">"Optimal Policy"</span>, <span class="st">"Value Function"</span>), horizontal_spacing<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal Policy heatmap</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>heatmap_policy <span class="op">=</span> go.Heatmap(</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    z<span class="op">=</span>policy,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    colorscale<span class="op">=</span>[[<span class="dv">0</span>, <span class="st">'red'</span>], [<span class="dv">1</span>, <span class="st">'blue'</span>]],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    zmin<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    zmax<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    colorbar<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Action"</span>, tickvals<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], ticktext<span class="op">=</span>[<span class="st">"Company A"</span>, <span class="st">"Company B"</span>], x<span class="op">=</span><span class="fl">0.46</span>),</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    hovertemplate<span class="op">=</span><span class="st">"Time Step: %</span><span class="sc">{y}</span><span class="st">&lt;br&gt;State: $%</span><span class="sc">{x}</span><span class="st">k&lt;br&gt;Action: %</span><span class="sc">{z}</span><span class="st">&lt;extra&gt;&lt;/extra&gt;"</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>fig.add_trace(heatmap_policy, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Value Function heatmap</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>heatmap_value <span class="op">=</span> go.Heatmap(</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    z<span class="op">=</span>V,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    colorscale<span class="op">=</span><span class="st">"Viridis"</span>,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    colorbar<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Expected Return ($k)"</span>, x<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    hovertemplate<span class="op">=</span><span class="st">"Time Step: %</span><span class="sc">{y}</span><span class="st">&lt;br&gt;State: $%</span><span class="sc">{x}</span><span class="st">k&lt;br&gt;Expected Return: $%</span><span class="sc">{z:.2f}</span><span class="st">k&lt;extra&gt;&lt;/extra&gt;"</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>fig.add_trace(heatmap_value, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Update layout</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Markov Decision Process: Investment Strategy"</span>,</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Update x and y axes</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(title_text<span class="op">=</span><span class="st">"State (Amount in thousands of dollars)"</span>, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span>i)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(title_text<span class="op">=</span><span class="st">"Time Step"</span>, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span>i)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="2d56c243-12e8-4b64-b611-ec68880bef67" class="plotly-graph-div" style="height:500px; width:1000px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("2d56c243-12e8-4b64-b611-ec68880bef67")) {                    Plotly.newPlot(                        "2d56c243-12e8-4b64-b611-ec68880bef67",                        [{"colorbar":{"ticktext":["Company A","Company B"],"tickvals":[0,1],"title":{"text":"Action"},"x":0.46},"colorscale":[[0,"red"],[1,"blue"]],"hovertemplate":"Time Step: %{y}\u003cbr\u003eState: $%{x}k\u003cbr\u003eAction: %{z}\u003cextra\u003e\u003c\u002fextra\u003e","z":[[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"zmax":1,"zmin":0,"type":"heatmap","xaxis":"x","yaxis":"y"},{"colorbar":{"title":{"text":"Expected Return ($k)"},"x":1},"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"hovertemplate":"Time Step: %{y}\u003cbr\u003eState: $%{x}k\u003cbr\u003eExpected Return: $%{z:.2f}k\u003cextra\u003e\u003c\u002fextra\u003e","z":[[0.0,1.4883199999999994,2.478719999999999,3.718079999999998,4.04992,5.062399999999999,6.0748799999999985,5.48688,6.27072,7.054559999999999,7.8384,8.622239999999998,9.406079999999998,6.176559999999999,6.651680000000001,7.126800000000001,7.60192,8.07704,8.55216,9.027280000000001,9.502400000000002,9.97752,10.452639999999999,10.927760000000001,11.40288,11.878000000000002,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,1.0735999999999994,2.147199999999999,3.220799999999998,3.4623999999999997,4.327999999999999,5.193599999999998,4.6704,5.3376,6.004799999999999,6.672000000000001,7.339199999999998,8.006399999999998,5.418399999999999,5.8352,6.252000000000001,6.668799999999999,7.0855999999999995,7.5024,7.919200000000001,8.336,8.7528,9.169599999999999,9.5864,10.0032,10.420000000000002,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.7279999999999999,1.4559999999999997,2.183999999999999,2.9119999999999995,3.6399999999999997,4.367999999999998,3.864,4.416,4.967999999999998,5.5200000000000005,6.071999999999997,6.623999999999998,4.576,4.928,5.28,5.632,5.983999999999999,6.335999999999999,6.688000000000001,7.04,7.3919999999999995,7.743999999999999,8.095999999999998,8.447999999999999,8.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.43999999999999995,0.8799999999999999,1.3199999999999994,1.7599999999999998,2.2,2.639999999999999,3.0799999999999996,3.5199999999999996,3.9599999999999986,4.4,4.839999999999998,5.279999999999998,3.6399999999999997,3.92,4.2,4.4799999999999995,4.759999999999999,5.039999999999999,5.32,5.6,5.879999999999999,6.159999999999998,6.439999999999998,6.719999999999998,7.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.19999999999999996,0.3999999999999999,0.5999999999999996,0.7999999999999998,1.0,1.1999999999999993,1.4,1.5999999999999996,1.7999999999999994,2.0,2.1999999999999993,2.3999999999999986,2.5999999999999996,2.8,3.0,3.1999999999999993,3.3999999999999986,3.5999999999999988,3.8,4.0,4.199999999999999,4.399999999999999,4.599999999999998,4.799999999999997,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]],"type":"heatmap","xaxis":"x2","yaxis":"y2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,0.35],"title":{"text":"State (Amount in thousands of dollars)"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Time Step"}},"xaxis2":{"anchor":"y2","domain":[0.6499999999999999,0.9999999999999999],"title":{"text":"State (Amount in thousands of dollars)"}},"yaxis2":{"anchor":"x2","domain":[0.0,1.0],"title":{"text":"Time Step"}},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Optimal Policy","x":0.175,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Value Function","x":0.825,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"}],"title":{"text":"Markov Decision Process: Investment Strategy"},"height":500,"width":1000},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('2d56c243-12e8-4b64-b611-ec68880bef67');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
</section>
<section id="interpreting-the-results" class="level2">
<h2 class="anchored" data-anchor-id="interpreting-the-results">Interpreting the Results</h2>
<section id="optimal-policy-chart" class="level3">
<h3 class="anchored" data-anchor-id="optimal-policy-chart">Optimal Policy Chart</h3>
<p>This heatmap shows the optimal investment strategy for each combination of time step and current investment amount. Red cells represent investing in Company A (low risk), while blue cells represent investing in Company B (high risk). Notice how the optimal strategy changes based on the current investment amount and the remaining time steps.</p>
<p><strong>Explanation for new students</strong>: This chart helps you visualize the best action to take at each point in time and for each possible investment amount. The color of each cell tells you which company to invest in: - Red: Invest in Company A (safer option) - Blue: Invest in Company B (riskier option)</p>
<p>As you look at the chart, pay attention to how the colors change. This shows that the best decision isn’t always the same – it depends on how much money you have and how many years are left to invest.</p>
</section>
<section id="value-function-chart" class="level3">
<h3 class="anchored" data-anchor-id="value-function-chart">Value Function Chart</h3>
<p>This heatmap displays the expected return (in thousands of dollars) for each combination of time step and current investment amount, assuming optimal decisions are made from that point forward. Brighter colors indicate higher expected returns.</p>
<p><strong>Explanation for new students</strong>: This chart shows you how much money you can expect to have at the end, depending on your current situation. Each cell’s color represents the expected amount of money: - Darker colors: Lower expected returns - Brighter colors: Higher expected returns</p>
<p>Notice how the colors generally get brighter as you move up (more time left) and to the right (more money to invest). This shows that having more time to invest and starting with more money typically leads to higher expected returns.</p>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ol type="1">
<li>The optimal policy is not static; it depends on both the current state (amount of money) and the time remaining.</li>
<li>There’s a trade-off between risk and reward, represented by the choice between Company A and Company B.</li>
<li>The value function quantifies the expected return under the optimal policy, given any starting state and time.</li>
</ol>
</section>
<section id="bellman-optimality-principle" class="level2">
<h2 class="anchored" data-anchor-id="bellman-optimality-principle">Bellman Optimality Principle</h2>
<p>The Bellman optimality principle states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</p>
<p>This principle simplifies complex decision problems by considering that the solution to the entire problem involves solving smaller sub-problems recursively. This is what allows us to solve MDPs efficiently using dynamic programming, working backward from the final time step to the initial one.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Finite horizon Markov Decision Processes provide a powerful framework for modeling sequential decision-making problems. By using dynamic programming and the Bellman optimality principle, we can compute optimal policies and value functions efficiently, even for complex problems with many states and actions.</p>
<p>These visualizations help us understand and interpret the solutions to MDP problems, making it easier to apply these concepts to various decision-making scenarios. Remember, in real-world applications, MDPs can have many more states, actions, and complex transition probabilities, but the core principles remain the same.</p>
<p>In practice, MDPs are used in various fields, including finance, robotics, inventory management, and more. Understanding these concepts provides a strong foundation for tackling real-world decision-making problems under uncertainty.</p>
</section>
<section id="lecture-notes" class="level2">
<h2 class="anchored" data-anchor-id="lecture-notes">Lecture notes</h2>
</section>
<section id="markov-decision-process-mdp" class="level1">
<h1>Markov Decision Process (MDP)</h1>
<ul>
<li><strong>Decision Epochs</strong>: <span class="math inline">\(N = \{0, 1, \ldots, T\}\)</span>, with <span class="math inline">\(T\)</span> finite.</li>
<li><strong>State Space</strong>: <span class="math inline">\(i \in S\)</span>, where <span class="math inline">\(S\)</span> is a discrete set of states.</li>
<li><strong>Action Space</strong>: For each state <span class="math inline">\(i\)</span>, the action space is <span class="math inline">\(A(i)\)</span>, where <span class="math inline">\(a \in A(i)\)</span>.</li>
<li><strong>Action Selection</strong>: Actions are selected according to a probability distribution <span class="math inline">\(g(\cdot) \in P(A(i))\)</span>, where an action <span class="math inline">\(a \in A(i)\)</span> is selected with probability <span class="math inline">\(g(a)\)</span>.</li>
<li><strong>Immediate Reward</strong>: The immediate reward at decision epoch <span class="math inline">\(n\)</span> for state <span class="math inline">\(i\)</span> and action <span class="math inline">\(a\)</span> is <span class="math inline">\(r_i^n(a)\)</span>. The reward <span class="math inline">\(r_i^T\)</span> is the salvage value at the final epoch <span class="math inline">\(T\)</span>.</li>
<li><strong>Transition Probabilities</strong>: The probability of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> at decision epoch <span class="math inline">\(n\)</span>, given action <span class="math inline">\(a\)</span>, is <span class="math inline">\(p_{ij}^n(a)\)</span>.</li>
<li>The MDP is defined by the tuple <span class="math inline">\(\{N, S, A(i), p_{ij}^n(a), r_i^n(a)\}\)</span>.</li>
</ul>
<section id="objective" class="level3">
<h3 class="anchored" data-anchor-id="objective">Objective:</h3>
<p>The goal is to solve the Markov Decision Problem (MDP) with the above components.</p>
</section>
</section>
<section id="history-and-decision-rules-in-a-markov-decision-process-mdp" class="level1">
<h1>History and Decision Rules in a Markov Decision Process (MDP)</h1>
<ul>
<li><p><strong>History up to time <span class="math inline">\(n\)</span></strong>: The history at decision epoch <span class="math inline">\(n\)</span> is denoted by <span class="math inline">\(h_n = (i_0, a_0, i_1, a_1, \dots, i_{n-1}, a_{n-1}, i_n)\)</span>, where <span class="math inline">\(i_t\)</span> is the state at time <span class="math inline">\(t\)</span> and <span class="math inline">\(a_t\)</span> is the action taken at time <span class="math inline">\(t\)</span>.</p></li>
<li><p><strong>Initial History</strong>: At decision epoch 0, the history is defined as <span class="math inline">\(H_0 = S\)</span>, meaning the history is simply the initial state space <span class="math inline">\(S\)</span>.</p></li>
<li><p><strong>History at Epoch <span class="math inline">\(n\)</span></strong>: For any <span class="math inline">\(n &gt; 0\)</span>, the history at time <span class="math inline">\(n\)</span> is <span class="math inline">\(H_n = H_{n-1} \times A \times S\)</span>, where <span class="math inline">\(H_{n-1}\)</span> is the history up to time <span class="math inline">\(n-1\)</span>, <span class="math inline">\(A\)</span> is the action space, and <span class="math inline">\(S\)</span> is the state space. The operator <span class="math inline">\(\times\)</span> represents the Cartesian product.</p></li>
</ul>
<section id="decision-rules" class="level3">
<h3 class="anchored" data-anchor-id="decision-rules">Decision Rules</h3>
<section id="deterministic-markovian-decision-rule-md" class="level4">
<h4 class="anchored" data-anchor-id="deterministic-markovian-decision-rule-md">1. Deterministic Markovian Decision Rule (MD)</h4>
<ul>
<li>A deterministic Markovian decision rule <span class="math inline">\(\sigma^n\)</span> is a mapping from states to actions: <span class="math display">\[
\sigma^n: S \to A
\]</span></li>
<li>For each state <span class="math inline">\(i \in S\)</span>, the action chosen by <span class="math inline">\(\sigma^n\)</span> at time <span class="math inline">\(n\)</span> is <span class="math inline">\(\sigma^n(i) \in A(i)\)</span>, where <span class="math inline">\(A(i)\)</span> is the set of actions available in state <span class="math inline">\(i\)</span>.</li>
</ul>
</section>
<section id="deterministic-history-dependent-decision-rule-hd" class="level4">
<h4 class="anchored" data-anchor-id="deterministic-history-dependent-decision-rule-hd">2. Deterministic History-Dependent Decision Rule (HD)</h4>
<ul>
<li>A deterministic history-dependent decision rule <span class="math inline">\(\sigma^n\)</span> is a mapping from the history at time <span class="math inline">\(n\)</span> to actions: <span class="math display">\[
\sigma^n: H_n \to A
\]</span></li>
<li>For each history <span class="math inline">\(h_n \in H_n\)</span>, the action chosen by <span class="math inline">\(\sigma^n\)</span> is <span class="math inline">\(\sigma^n(h_n) \in A(i_n)\)</span>, where <span class="math inline">\(i_n\)</span> is the current state in the history <span class="math inline">\(h_n\)</span>.</li>
</ul>
</section>
<section id="randomized-markovian-decision-rule-mr" class="level4">
<h4 class="anchored" data-anchor-id="randomized-markovian-decision-rule-mr">3. Randomized Markovian Decision Rule (MR)</h4>
<ul>
<li>A randomized Markovian decision rule <span class="math inline">\(q^n\)</span> specifies a probability distribution over actions for each state: <span class="math display">\[
q^n: S \to P(A(i))
\]</span></li>
<li>For each state <span class="math inline">\(i \in S\)</span>, <span class="math inline">\(q^n(i)\)</span> is a probability distribution over the action set <span class="math inline">\(A(i)\)</span>, meaning an action is selected according to <span class="math inline">\(q^n(i)\)</span>.</li>
</ul>
</section>
<section id="randomized-history-dependent-decision-rule-hr" class="level4">
<h4 class="anchored" data-anchor-id="randomized-history-dependent-decision-rule-hr">4. Randomized History-Dependent Decision Rule (HR)</h4>
<ul>
<li>A randomized history-dependent decision rule <span class="math inline">\(q^n\)</span> specifies a probability distribution over actions for each history: <span class="math display">\[
q^n: H_n \to P(A(i_n))
\]</span></li>
<li>For each history <span class="math inline">\(h_n \in H_n\)</span>, <span class="math inline">\(q^n(h_n)\)</span> is a probability distribution over the action set <span class="math inline">\(A(i_n)\)</span>, meaning an action is selected according to <span class="math inline">\(q^n(h_n)\)</span>.</li>
</ul>
</section>
</section>
</section>
<section id="policies-in-a-markov-decision-process-mdp" class="level1">
<h1>Policies in a Markov Decision Process (MDP)</h1>
<section id="definition-of-a-policy" class="level3">
<h3 class="anchored" data-anchor-id="definition-of-a-policy">Definition of a Policy</h3>
<ul>
<li><p>A <strong>policy</strong> <span class="math inline">\(\sigma = (\sigma^0, \sigma^1, \dots, \sigma^{T-1})\)</span> is a sequence of decision rules, one for each decision epoch <span class="math inline">\(n \in \{0, 1, \dots, T-1\}\)</span>, where each decision rule <span class="math inline">\(\sigma^n\)</span> belongs to a set of decision rules <span class="math inline">\(D^K_n\)</span>, defined by some class <span class="math inline">\(K\)</span> of decision rules. This means: <span class="math display">\[
\sigma^n \in D^K_n \quad \forall n \in \{0, 1, \dots, T-1\}
\]</span></p></li>
<li><p>The class <span class="math inline">\(K\)</span> refers to the type of decision rules that the policy can adopt. For example:</p>
<ul>
<li>If <span class="math inline">\(K\)</span> refers to <strong>deterministic Markovian</strong> rules, then each <span class="math inline">\(\sigma^n\)</span> is a mapping from the state space <span class="math inline">\(S\)</span> to the action space <span class="math inline">\(A(i)\)</span>.</li>
<li>If <span class="math inline">\(K\)</span> refers to <strong>randomized history-dependent</strong> rules, then each <span class="math inline">\(\sigma^n\)</span> is a mapping from the history space <span class="math inline">\(H_n\)</span> to a probability distribution over the action space <span class="math inline">\(A(i_n)\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="set-of-policies" class="level3">
<h3 class="anchored" data-anchor-id="set-of-policies">Set of Policies</h3>
<ul>
<li>The <strong>set of all policies</strong> of class <span class="math inline">\(K\)</span>, denoted by <span class="math inline">\(\Pi^K\)</span>, is the Cartesian product of decision rule sets at each epoch: <span class="math display">\[
\Pi^K = D^K_0 \times D^K_1 \times \dots \times D^K_{T-1}
\]</span></li>
<li>This means that a policy <span class="math inline">\(\sigma \in \Pi^K\)</span> is a sequence of decision rules chosen from the class <span class="math inline">\(K\)</span>, with one decision rule for each decision epoch.</li>
</ul>
</section>
<section id="example-classes-of-decision-rules" class="level3">
<h3 class="anchored" data-anchor-id="example-classes-of-decision-rules">Example Classes of Decision Rules:</h3>
<ol type="1">
<li><p><strong>Deterministic Markovian (MD)</strong>: If <span class="math inline">\(K\)</span> is the class of deterministic Markovian rules, then <span class="math inline">\(D^K_n = \{ \sigma^n: S \to A(i) \}\)</span>, meaning each decision rule selects a deterministic action based solely on the current state.</p></li>
<li><p><strong>Randomized Markovian</strong>: If <span class="math inline">\(K\)</span> is the class of randomized Markovian rules, then <span class="math inline">\(D^K_n = \{ q^n: S \to P(A(i)) \}\)</span>, meaning each decision rule selects an action according to a probability distribution based on the current state.</p></li>
<li><p><strong>Deterministic History-Dependent (HD)</strong>: If <span class="math inline">\(K\)</span> is the class of deterministic history-dependent rules, then <span class="math inline">\(D^K_n = \{ \sigma^n: H_n \to A(i_n) \}\)</span>, meaning each decision rule selects an action based on the full history up to time <span class="math inline">\(n\)</span>.</p></li>
<li><p><strong>Randomized History-Dependent</strong>: If <span class="math inline">\(K\)</span> is the class of randomized history-dependent rules, then <span class="math inline">\(D^K_n = \{ q^n: H_n \to P(A(i_n)) \}\)</span>, meaning each decision rule selects an action according to a probability distribution based on the full history up to time <span class="math inline">\(n\)</span>.</p></li>
</ol>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>A <strong>policy</strong> is a sequence of decision rules, where each rule belongs to a class <span class="math inline">\(K\)</span> of decision rules.</li>
<li>The <strong>set of all policies</strong> of class <span class="math inline">\(K\)</span>, denoted <span class="math inline">\(\Pi^K\)</span>, is the Cartesian product of decision rule sets at each decision epoch.</li>
<li>Different classes of decision rules (e.g., deterministic Markovian, randomized history-dependent) lead to different types of policies.</li>
</ul>
</section>
</section>
<section id="stochastic-process-induced-by-a-policy-in-an-mdp" class="level1">
<h1>Stochastic Process Induced by a Policy in an MDP</h1>
<p>Let <span class="math inline">\(X_n\)</span>, <span class="math inline">\(Y_n\)</span>, and <span class="math inline">\(Z_n\)</span> represent the state, action, and history at time <span class="math inline">\(n\)</span>, respectively.</p>
<ul>
<li><strong><span class="math inline">\(X_n\)</span></strong>: State at time <span class="math inline">\(n\)</span>.</li>
<li><strong><span class="math inline">\(Y_n\)</span></strong>: Action taken at time <span class="math inline">\(n\)</span>.</li>
<li><strong><span class="math inline">\(Z_n\)</span></strong>: History up to time <span class="math inline">\(n\)</span>, <span class="math inline">\(Z_n = (i_0, a_0, i_1, a_1, \dots, i_{n-1}, a_{n-1}, i_n)\)</span>.</li>
</ul>
<p>Let <span class="math inline">\(\sigma \in \Pi^{HR}\)</span> denote a <strong>policy</strong> from the set of <strong>history-dependent randomized policies</strong> <span class="math inline">\(\Pi^{HR}\)</span>. This means that the policy <span class="math inline">\(\sigma\)</span> selects actions based on the entire history up to time <span class="math inline">\(n\)</span>. The policy <span class="math inline">\(\sigma\)</span> and the initial state <span class="math inline">\(i_0\)</span> induce a <strong>stochastic process</strong>:</p>
<section id="stochastic-process-induced-by-a-policy" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-process-induced-by-a-policy">Stochastic Process Induced by a Policy</h3>
<p>Given the policy <span class="math inline">\(\sigma \in \Pi^{HR}\)</span> and the initial state <span class="math inline">\(i_0 \in S\)</span>, the evolution of the MDP is a stochastic process:</p>
<ol type="1">
<li><p><strong>Initial State</strong>: The process starts at state <span class="math inline">\(X_0 = i_0\)</span>.</p></li>
<li><p><strong>Action Selection</strong>:</p>
<ul>
<li>At time <span class="math inline">\(n\)</span>, the policy <span class="math inline">\(\sigma^n\)</span> selects an action <span class="math inline">\(Y_n\)</span> based on the history <span class="math inline">\(Z_n\)</span>. Since <span class="math inline">\(\sigma \in \Pi^{HR}\)</span>, it maps the history <span class="math inline">\(Z_n\)</span> to a probability distribution over actions: <span class="math display">\[
Y_n \sim q^n(Z_n), \quad \text{where} \quad q^n: H_n \to P(A(i_n))
\]</span> This means that at each time <span class="math inline">\(n\)</span>, the policy <span class="math inline">\(\sigma^n\)</span> selects an action <span class="math inline">\(Y_n\)</span> according to the probability distribution <span class="math inline">\(q^n(Z_n)\)</span>, which depends on the entire history <span class="math inline">\(Z_n\)</span>.</li>
</ul></li>
<li><p><strong>State Transition</strong>:</p>
<ul>
<li>After action <span class="math inline">\(Y_n\)</span> is taken in state <span class="math inline">\(X_n\)</span>, the system transitions to a new state <span class="math inline">\(X_{n+1}\)</span> according to the transition probability <span class="math inline">\(p_{ij}^n(Y_n)\)</span>, where: <span class="math display">\[
P(X_{n+1} = j \mid X_n = i, Y_n = a) = p_{ij}^n(a)
\]</span></li>
<li>This stochastic transition depends on the current state <span class="math inline">\(X_n = i\)</span> and the selected action <span class="math inline">\(Y_n = a\)</span>.</li>
</ul></li>
<li><p><strong>History Update</strong>:</p>
<ul>
<li>The history <span class="math inline">\(Z_{n+1}\)</span> is updated to include the new state and action: <span class="math display">\[
Z_{n+1} = (Z_n, Y_n, X_{n+1}) = (i_0, a_0, i_1, a_1, \dots, i_n, a_n, i_{n+1})
\]</span></li>
<li>The updated history is then used by the policy <span class="math inline">\(\sigma^{n+1}\)</span> to select the action at the next time step.</li>
</ul></li>
</ol>
</section>
<section id="resulting-stochastic-process" class="level3">
<h3 class="anchored" data-anchor-id="resulting-stochastic-process">Resulting Stochastic Process</h3>
<p>Thus, the tuple <span class="math inline">\(\{X_n, Y_n, Z_n\}_{n=0}^{T-1}\)</span> forms a <strong>stochastic process</strong> where: - <span class="math inline">\(X_n\)</span> evolves according to the transition probabilities <span class="math inline">\(p_{ij}^n(a)\)</span>, - <span class="math inline">\(Y_n\)</span> is chosen based on the policy <span class="math inline">\(\sigma^n\)</span> from the set of history-dependent randomized policies, and - <span class="math inline">\(Z_n\)</span> tracks the history of states and actions up to time <span class="math inline">\(n\)</span>.</p>
<p>The stochastic process induced by the policy <span class="math inline">\(\sigma \in \Pi^{HR}\)</span> and initial state <span class="math inline">\(i_0\)</span> is governed by the evolution of states, actions, and history over time according to the rules of the MDP.</p>
</section>
<section id="summary-1" class="level3">
<h3 class="anchored" data-anchor-id="summary-1">Summary:</h3>
<ul>
<li>A policy <span class="math inline">\(\sigma \in \Pi^{HR}\)</span> induces a stochastic process starting from the initial state <span class="math inline">\(i_0\)</span>.</li>
<li>The stochastic process evolves through a sequence of states <span class="math inline">\(X_n\)</span>, actions <span class="math inline">\(Y_n\)</span>, and histories <span class="math inline">\(Z_n\)</span>.</li>
<li>The actions are selected based on the full history up to time <span class="math inline">\(n\)</span>, and the state transitions are governed by the transition probabilities of the MDP.</li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>